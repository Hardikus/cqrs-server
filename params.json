{"name":"Cqrs-server","tagline":"An opinionated Clojure CQRS/ES implementation using Onyx, Datomic, DynamoDB, Kafka and Zookeeper.","body":"# Introduction to _cqrs-server_\r\n\r\nAn opinionated Clojure CQRS/ES implementation using [Onyx](https://github.com/MichaelDrogalis/onyx), [Datomic](http://www.datomic.com/), [DynamoDB](http://aws.amazon.com/dynamodb/), [Kafka](http://kafka.apache.org/) and [Zookeeper](http://zookeeper.apache.org/).\r\n\r\n## The problem\r\n\r\nBest to start with the core problem at hand:\r\n\r\n> The current relational database is too limiting. We're dropping all sorts of interesting data on the ground because we don't have suitable pigeonholes to put it into.\r\n\r\nAny system that has enough interesting interaction happening to it faces this problem. The data that we do end up putting into our database tends to inconsistent design. It is then risky to change down the line because it gets treated as our _[system of record](http://en.wikipedia.org/wiki/System_of_record)_.\r\n\r\nTo this end, we've been doing research into _Event Sourcing_ (ES). This led to _Command/Query Responsibility Segregation_ (CQRS) and touched on the area of _Domain Driven Design_ (DDD).\r\n\r\n## Introduction to Event Sourcing (ES)\r\n\r\nAn event is a past tense archive record about what has happened. Examples are `UserLoggedIn`, `AddedToCart`, `CheckedOut`, `OrderShipped`, `OrderDelivered` and `ProductFaultLogged`.\r\n\r\nThe idea is that if you maintain a list of these events, you have a canonical source that you can build arbitrary aggregate views from. You can completely obviate the need for cache invalidation - Treat the events as a stream and update all the various sources as events filter through.\r\n\r\nThe past tense is important to distinguish Events from Commands like `UserLogin` or `AddToCart`. A Command implies that the user is attempting an action - the system can still make some kind of decision, validation or even refusal to process. This is not the case in past tense events as you cannot mutate history.\r\n\r\nYou store the list of events into an Event Store and use that as the canonical state of the system. You derive views in other shapes optimized for reading. \r\n\r\nA key benefit is that you can introduce new derived views. Leverage existing events and populate the views with historical data as if they were always there.\r\n\r\n\r\n## Introduction to Command/Query Responsibility Segregation\r\n\r\nMost of the reading material will describe CQRS through Object Oriented lenses. This introduces all sorts complexity - a lot to do with command responsibility managers and logical aggregate objects. We've distilled the essence of CQRS into:\r\n\r\n> \"Separate your writes and queries\"\r\n\r\nThis implies that you create an archive in the best form for writes and a separate storage convenient for reads.\r\n\r\nWhile we're distilling things, let's look at some useful word definitions (courtesy of [Merriam Webster Dictionary](http://www.merriam-webster.com)):\r\n\r\n> **Command (verb)**: to direct authoritatively\r\n>\r\n> **Query (noun)**: a question or a request for information about something\r\n>\r\n> **Aggregate (adj)**: formed by the collection of units or particles into a body, mass, or amount\r\n\r\nThe Command has a very distinct role in that it can make decisions about what events end up happening. `RegisterUser`, for example, might produce a `UserRegistered` or `UserRegistrationFailed` event.\r\n\r\nThe impedance mismatch usually associated with relational databases diminishes when you've split your database into a write-optimized archive and various read-optimized aggregates.\r\n\r\n## Implementation specifics\r\n\r\n![CQRS Diagram](https://www.lucidchart.com/publicSegments/view/54daedae-735c-48b6-a31a-41210a0082c6/image.png)\r\n\r\n### Onyx\r\n\r\nOnyx is a masterless distributed computation system. Effectively becoming the glue of the system and managing connections between the components.\r\n\r\nIt directs an arbitrary number of peers and allows the _cqrs-server_ to scale out by adding new nodes. The configuration is also flexible. It's simple to replace Kafka with another queueing mechanism or introduce more complex batch jobs.\r\n\r\nThe nature of Onyx is that it's a distributed system. Thus, we have to assume that duplicate Commands and Events will process. We need to ensure that side-effecting changes are idempotent. _cqrs-server_ achieves this by doing a few things under the hood:\r\n\r\n - Attach a uuid to the command before sending it into the queue\r\n - Attach a current basis-t of the datomic database value as at command generation\r\n - Derive event uuids from the command uuid ([namespaced v5 uuids](https://github.com/danlentz/clj-uuid/blob/86c9feb84c2175466f1c2784b3f740f523a84302/src/clj_uuid.clj#L321-L326)) so that they are deterministic\r\n - Ensure that the event store takes care of duplicate event writes\r\n - Ensure that the aggregate store treats duplicate event transactions as no-ops\r\n\r\nWith Onyx we can define the workflow:\r\n\r\n![Workflow, visualized](https://www.lucidchart.com/publicSegments/view/54daef70-57e8-4898-8d7d-12390a0082c6/image.png)\r\n\r\nIn code, this is just a vector of tuples:\r\n```clojure\r\n(def command-workflow\r\n [[:command/in-queue :command/coerce]\r\n  [:command/coerce :command/process]\r\n  [:command/process :event/out-queue]\r\n  [:event/in-queue :event/prepare-store]\r\n  [:event/prepare-store :event/store]\r\n  [:event/in-queue :event/aggregator]\r\n  [:event/aggregator :event/store-aggregate]])\r\n```\r\n\r\nThis gives Onyx enough information to know where to send batches, but not enough to know what the places are. For that, it needs a catalog. In the case of _cqrs-server_ it looks like: _(we've elided many specifics for the clarity)_\r\n\r\n```clojure\r\n=> (pprint catalog)\r\n[{:onyx/name :command/in-queue,\r\n  :onyx/medium :kafka,\r\n  :onyx/ident :kafka/read-messages,\r\n  :onyx/type :input,\r\n  :kafka/zookeeper \"127.0.0.1:2181\",\r\n  :kafka/topic \"command-queue\"}\r\n  \r\n {:onyx/name :command/coerce,\r\n  :onyx/type :function,\r\n  :onyx/fn :cqrs-server.cqrs/command-coerce*}\r\n  \r\n {:onyx/name :command/process,\r\n  :onyx/type :function,\r\n  :onyx/fn :cqrs-server.cqrs/process-command*}\r\n  \r\n {:onyx/name :event/out-queue,\r\n  :onyx/medium :kafka,\r\n  :onyx/ident :kafka/write-messages,\r\n  :onyx/type :output,\r\n  :kafka/topic \"event-queue\",\r\n  :kafka/brokers \"127.0.0.1:9092\"}\r\n  \r\n {:onyx/name :event/in-queue,\r\n  :onyx/ident :kafka/read-messages,\r\n  :onyx/medium :kafka,\r\n  :onyx/type :input,\r\n  :kafka/zookeeper \"127.0.0.1:2181\",\r\n  :kafka/topic \"event-queue\"}\r\n  \r\n {:onyx/name :event/prepare-store,\r\n  :onyx/type :function,\r\n  :onyx/fn :cqrs-server.cqrs/prepare-store}\r\n  \r\n {:onyx/name :event/store,\r\n  :onyx/ident :dynamodb/commit-tx,\r\n  :onyx/type :output,\r\n  :onyx/medium :dynamodb,\r\n  :dynamodb/table :events,\r\n  :dynamodb/config\r\n  {:access-key \"aws-access-key\",\r\n   :secret-key \"aws-secret-key\",\r\n   :endpoint \"http://localhost:8000\"}}\r\n   \r\n {:onyx/name :event/aggregator,\r\n  :onyx/type :function,\r\n  :onyx/fn :cqrs-server.cqrs/aggregate-event*}\r\n  \r\n {:onyx/name :event/store-aggregate,\r\n  :onyx/ident :datomic/commit-tx,\r\n  :onyx/type :output,\r\n  :onyx/medium :datomic-tx,\r\n  :datomic/uri \"datomic:mem://cqrs\"}]\r\n```\r\n\r\nOnyx manages the lifecycle of each of the components defined in the catalog. As messages progress through the workflow it uses the `:onyx/name` to lookup the component.\r\n\r\n### Apache Kafka\r\n\r\nKafka is a high-throughput, log-based publish-subscribe messaging system.\r\n\r\nIt holds on to the messages after consumption. This makes it well suited for failure recovery by replaying the recent commands or events. In the diagram above Kafka is the implementation of the Command and Event queues.\r\n\r\n### Amazon DynamoDB\r\n\r\nDynamoDB is a flexible and scalable K/V store which we're using as the Event Store. It doesn't matter where this goes as long as it's reliable and allows you to query a subset of the events by a given date range.\r\n\r\nTo take care of duplicates, we rewrite the same event into Dynamo using the event uuid as a key. This will overwrite the duplicate event, but won't change anything.\r\n\r\n### Datomic\r\n\r\nDatomic is a transactional, distributed Entity, Attribute, Value, Tx (EAVT) database. It provides expressive querying facilities, has a single-node transactor and near-arbitrary read scalability.\r\n\r\nThe single-node transactor is key to de-duplicating the event aggregation. We created an `:idempotent-tx` transactor function that checks:\r\n\r\n - If a tx is _already_ tagged with the given `:event/uuid`, convert tx to a no-op\r\n - If a tx is _not_ tagged, tag this tx with the `:event/uuid` and commit.\r\n\r\nAnother concern is that of duplicated commands. A command could produce a different set of events when one set of aggregates reaches the transactor before it gets to run.\r\n\r\nTo prevent inconsistent results, we tag the command with the current `basis-t`. This roots our command to a specific immutable database value.  The command should then always evaluate to the same result if it makes decisions based on the Datomic aggregate as of `basis-t`.\r\n\r\n## Running _cqrs-server_\r\n\r\nWe've covered some of the larger design decisions. Let's delve into the _cqrs-server_ specific implementation.\r\n\r\nFirst, download and unzip dynamodb local from <http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Tools.DynamoDBLocal.html> and run:\r\n\r\n```bash\r\njava -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar\r\n```\r\n\r\nNext, download Kafka from <http://kafka.apache.org/downloads.html>, unzip and run both:\r\n```bash\r\nbin/zookeeper-server-start.sh config/zookeeper.properties\r\nbin/kafka-server-start.sh config/server.properties\r\n```\r\nFinally, clone _cqrs-server_ and run:\r\n```bash\r\ngit clone https://github.com/Yuppiechef/cqrs-server.git\r\ncd cqrs-server\r\nlein repl\r\n```\r\nAfter a bit of compiling you should have a REPL waiting, run the following function:\r\n```clojure\r\n(start)\r\n```\r\nIt should give you a `\"Setup Complete\"` message once it's done and ready to start taking commands. Go open the `src/cqrs_server/module.clj` file and have a look.\r\n\r\n## Playing around\r\nThe `module.clj` should give you a feel for roughly how you would add your own logic into the system:\r\n - Define an aggregate schema for datomic\r\n - Install the command schema's \r\n - Implement `cqrs/process-command` for each command\r\n - Implement `cqrs/aggregate-event` for each event\r\n\r\n`cqrs/aggregate-event` is optional. You will notice that there is not aggregation for the `:user/register-failed` event. This will still record the event in Dynamo, but have no read view.\r\n\r\n_**Quick aside**: For convenience, we will be using the `=>` in the samples below to show the REPL prompt and prefix the result of the calls with `;;`_\r\n\r\nBack in your REPL, try register a user and check that he exists:\r\n```clojure\r\n=> (send-command :user/register {:name \"Bob\" :age 31})\r\n```\r\nThis will fire off the user registration - check that it worked:\r\n```clojure\r\n=> (d/q '[:find [?e ...] :where [?e :user/name]] (d/db (d/connect datomic-uri)))\r\n;; [17592186045422]\r\n=> (map #(d/touch (d/entity (d/db (d/connect datomic-uri)) %)) *1)\r\n;; ({:base/uuid #uuid \"54d8fc2e-6c1f-4fb6-93f9-bef9536a9f7d\", :user/age 31, :user/name \"Bob\", :db/id 17592186045422})\r\n```\r\nYou can then check your dynamodb event store:\r\n```clojure\r\n=> (far/scan dynamodb-cred :events)\r\n;; [{:date 1423510153513N, :data #<byte[] ...>, :basis-t 1000N, :id \"be856c9c-0bf8-5ccc-bec1-bfa0f5a7e983\", :type \"user/registered\"}]\r\n```\r\n\r\nYou can go ahead and play around with sending the other commands in the `module.clj` and see how they affect things. Also try create new commands and event aggregators - there's not much to it.\r\n\r\n## Where's my query?\r\n\r\nThe astute reader will notice that this discussion has focussed on the Command part of CQRS and not so much on the Query part. This is because once you have your aggregate view, you're done.\r\n\r\nAny part of your system that needs to read can directly consume the aggregate views with no need to interact with the _cqrs-server_.\r\n\r\n## Conclusion\r\n\r\nWe have shown a functional distillation of CQRS. We've composed various pieces of software to build a solid foundation for a flexible distributed system. _cqrs-server_ provides the basic framework needed for a CQRS-based system.\r\n\r\n## Further Reading\r\nSome related reading that influenced this design, in no particular order:\r\n\r\n - [Reimagining our front-end architecture at Yuppiechef.com](https://medium.com/@ohthatjonathan/reimagining-our-front-end-architecture-at-yuppiechef-com-b1782fc4c392)\r\n - [CQS on Wikipedia](http://en.wikipedia.org/wiki/Command-query_separation)\r\n - [CQRS Documents by Greg Young](https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf)\r\n - [Martin Fowler: Command Query Separation](http://martinfowler.com/bliki/CommandQuerySeparation.html)\r\n - [Event Sourcing Basics](http://docs.geteventstore.com/introduction/event-sourcing-basics/)\r\n - [Event Sourcing for Functional Programmers](http://danielwestheide.com/talks/flatmap2013/slides/index.html)\r\n - [Martin Fowler: Event Sourcing](http://martinfowler.com/eaaDev/EventSourcing.html)\r\n - [DDD, Event Sourcing, and CQRS Tutorial: design](http://cqrs.nu/tutorial/cs/01-design)\r\n - [CQRS Journey](http://msdn.microsoft.com/en-us/library/jj554200.aspx)\r\n  ","google":"UA-59536081-1","note":"Don't delete this file! It's used internally to help with page regeneration."}